{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_n-3qhTpXjsy"
   },
   "source": [
    "# Importing libraries and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bpiddPjsl_4Q"
   },
   "outputs": [],
   "source": [
    "# import key libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#import gensim\n",
    "#from gensim.utils import simple_preprocess\n",
    "#from gensim.parsing.preprocessing import STOPWORDS\n",
    "import plotly.express as px\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import one_hot,Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8QiTczEunJNx"
   },
   "outputs": [],
   "source": [
    "# load the stock news data\n",
    "stock_df = pd.read_csv(r\"data\\stock_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DB2gQ1w4nR-P"
   },
   "outputs": [],
   "source": [
    "# Let's view the dataset \n",
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mEVxeFq5Zlsd"
   },
   "outputs": [],
   "source": [
    "# dataframe information\n",
    "stock_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iH5W6t7fZoQS"
   },
   "outputs": [],
   "source": [
    "# check for null values\n",
    "stock_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgkZHO3CPnsp"
   },
   "source": [
    "# Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BAWaIZEd6cYj"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbWfzII36cWc"
   },
   "outputs": [],
   "source": [
    "Test = '$I love AI & Machine learning!!'\n",
    "Test_punc_removed = [char for char in Test if char not in string.punctuation]\n",
    "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "Test_punc_removed_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvhT_EjX6cUQ"
   },
   "outputs": [],
   "source": [
    "Test = 'Good morning beautiful people :)... #I am having fun learning Finance with Python!!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WM76PxJ36cSQ"
   },
   "outputs": [],
   "source": [
    "Test_punc_removed = [char for char in Test if char not in string.punctuation]\n",
    "Test_punc_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MmG7ijks6aqt"
   },
   "outputs": [],
   "source": [
    "# Join the characters again to form the string.\n",
    "Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "Test_punc_removed_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HE4dRfZG_O8O"
   },
   "outputs": [],
   "source": [
    "# Let's define a function to remove punctuations\n",
    "def remove_punc(message):\n",
    "    Test_punc_removed = [char for char in message if char not in string.punctuation]\n",
    "    Test_punc_removed_join = ''.join(Test_punc_removed)\n",
    "\n",
    "    return Test_punc_removed_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KW-IgnK-dV4"
   },
   "outputs": [],
   "source": [
    "# Let's remove punctuations from our dataset \n",
    "stock_df['Text Without Punctuation'] = stock_df['Text'].apply(remove_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OU1Ubd45_3t-"
   },
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GklP2yj5Ab_D"
   },
   "outputs": [],
   "source": [
    "stock_df['Text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ohgg1iNHAe4W"
   },
   "outputs": [],
   "source": [
    "stock_df['Text Without Punctuation'][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2fMUtl66ztE"
   },
   "source": [
    "# Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmEkZj3k69S7"
   },
   "outputs": [],
   "source": [
    "# download stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HyDMuSWA69Y2"
   },
   "outputs": [],
   "source": [
    "# Obtain additional stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','stock','today','week','year'])\n",
    "# stop_words.extend(['from', 'subject', 're', 'edu', 'use','will','aap','co','day','user','stock','today','week','year', 'https'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jy1n42a069Vr"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords and remove short words (less than 2 characters)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if len(token) >= 3 and token not in stop_words:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zv1Igmf87a5I"
   },
   "outputs": [],
   "source": [
    "# apply pre-processing to the text column\n",
    "stock_df['Text Without Punc & Stopwords'] = stock_df['Text Without Punctuation'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OCgsA4vG9kxj"
   },
   "outputs": [],
   "source": [
    "stock_df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-WDumvb93wC"
   },
   "outputs": [],
   "source": [
    "stock_df['Text Without Punc & Stopwords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bYql7en7hcs"
   },
   "outputs": [],
   "source": [
    "# join the words into a string\n",
    "# stock_df['Processed Text 2'] = stock_df['Processed Text 2'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9JqZBzf9CEY"
   },
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7RUFuufl-Hx8"
   },
   "source": [
    "# WordClouding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cJAYGiUwg-Ez"
   },
   "outputs": [],
   "source": [
    "# join the words into a string\n",
    "stock_df['Text Without Punc & Stopwords Joined'] = stock_df['Text Without Punc & Stopwords'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3ErWl2jf7hYS"
   },
   "outputs": [],
   "source": [
    "# plot the word cloud for text with positive sentiment\n",
    "plt.figure(figsize = (20, 20)) \n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800).generate(\" \".join(stock_df[stock_df['Sentiment'] == 1]['Text Without Punc & Stopwords Joined']))\n",
    "plt.imshow(wc, interpolation = 'bilinear');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q2m28Qrtds_n"
   },
   "source": [
    "# Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eB8NRW0fIQRj"
   },
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u93O9tHHiRfd"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lgVLvPBLiZGb"
   },
   "outputs": [],
   "source": [
    "# word_tokenize is used to break up a string into words\n",
    "print(stock_df['Text Without Punc & Stopwords Joined'][0])\n",
    "print(nltk.word_tokenize(stock_df['Text Without Punc & Stopwords Joined'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r20ny06ECP1B"
   },
   "outputs": [],
   "source": [
    "# Obtain the maximum length of data in the document\n",
    "# This will be later used when word embeddings are generated\n",
    "maxlen = -1\n",
    "for doc in stock_df['Text Without Punc & Stopwords Joined']:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    if(maxlen < len(tokens)):\n",
    "        maxlen = len(tokens)\n",
    "print(\"The maximum number of words in any document is:\", maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ffCdD2BZjFyP"
   },
   "outputs": [],
   "source": [
    "tweets_length = [ len(nltk.word_tokenize(x)) for x in stock_df['Text Without Punc & Stopwords Joined'] ]\n",
    "tweets_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZDtUZMRVcD7I"
   },
   "outputs": [],
   "source": [
    "# Plot the distribution for the number of words in a text\n",
    "fig = px.histogram(x = tweets_length, nbins = 50)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zi3neznbP_QT"
   },
   "source": [
    "# Tokenizing and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZHGI_qYj6Iw"
   },
   "outputs": [],
   "source": [
    "stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "alPKDD0P7a28"
   },
   "outputs": [],
   "source": [
    "# Obtain the total words present in the dataset\n",
    "list_of_words = []\n",
    "for i in stock_df['Text Without Punc & Stopwords']:\n",
    "    for j in i:\n",
    "        list_of_words.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FN75gigvYVxc"
   },
   "outputs": [],
   "source": [
    "list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IN5g6jTF7a1I"
   },
   "outputs": [],
   "source": [
    "# Obtain the total number of unique words\n",
    "total_words = len(list(set(list_of_words)))\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lUgEf-SZ7R7c"
   },
   "outputs": [],
   "source": [
    "# split the data into test and train \n",
    "X = stock_df['Text Without Punc & Stopwords']\n",
    "y = stock_df['Sentiment']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgFE9ss1JmCw"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NeoabpYjnS3o"
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UcyTkiaZlQCK"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kme-IYsM6uJa"
   },
   "outputs": [],
   "source": [
    "# Create a tokenizer to tokenize the words and create sequences of tokenized words\n",
    "tokenizer = Tokenizer(num_words = total_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Training data\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# Testing data\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tGqwPkoiXea"
   },
   "outputs": [],
   "source": [
    "train_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e3joXOZeieD0"
   },
   "outputs": [],
   "source": [
    "test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TEWCpgXYNF1r"
   },
   "outputs": [],
   "source": [
    "print(\"The encoding for document\\n\", X_train[1:2],\"\\n is: \", train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URy4Wkai_Qh3"
   },
   "outputs": [],
   "source": [
    "# Add padding to training and testing\n",
    "padded_train = pad_sequences(train_sequences, maxlen = 29, padding = 'post', truncating = 'post')\n",
    "padded_test = pad_sequences(test_sequences, maxlen = 29, truncating = 'post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D60mkoZvSG5D"
   },
   "outputs": [],
   "source": [
    "for i, doc in enumerate(padded_train[:3]):\n",
    "     print(\"The padded encoding for document:\", i+1,\" is:\", doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlUjvz5UPZm0"
   },
   "outputs": [],
   "source": [
    "# Convert the data to categorical 2D representation\n",
    "y_train_cat = to_categorical(y_train, 2)\n",
    "y_test_cat = to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqPAOMmfPhxm"
   },
   "outputs": [],
   "source": [
    "y_train_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MYO-dWSjIHq"
   },
   "outputs": [],
   "source": [
    "y_test_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "und7m_TDjGf3"
   },
   "outputs": [],
   "source": [
    "y_train_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3k_ZJfGLjQhc"
   },
   "outputs": [],
   "source": [
    "# Sequential Model\n",
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model.add(Embedding(total_words, output_dim = 512))\n",
    "\n",
    "# Bi-Directional RNN and LSTM\n",
    "model.add(LSTM(256))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(2,activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KhbQTkvfjNVi"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "model.fit(padded_train, y_train_cat, batch_size = 32, validation_split = 0.2, epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J9UIM9ugjrb0"
   },
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7qRUkys-BSuQ"
   },
   "outputs": [],
   "source": [
    "# make prediction\n",
    "pred = model.predict(padded_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qh66RfZgF7ln"
   },
   "outputs": [],
   "source": [
    "# make prediction\n",
    "prediction = []\n",
    "for i in pred:\n",
    "    prediction.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Z3ka_OTQK6G"
   },
   "outputs": [],
   "source": [
    "# list containing original values\n",
    "original = []\n",
    "for i in y_test_cat:\n",
    "    original.append(np.argmax(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sxjH9_yZQj_m"
   },
   "outputs": [],
   "source": [
    "# acuracy score on text data\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(original, prediction)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7rpgzbSqHfR4"
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(original, prediction)\n",
    "sns.heatmap(cm, annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzEEVItSquw_"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# embedding layer\n",
    "model.add(Embedding(total_words, output_dim = 256))\n",
    "\n",
    "# Bi-Directional RNN and LSTM\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "# Dense layers\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stocks Sentiment Analysis Using AI - Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
